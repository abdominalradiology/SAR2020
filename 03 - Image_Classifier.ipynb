{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAR 2020 - Image Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTc1Wq1KGW1SPSzLyX2lPj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twloehfelm/SAR2020/blob/master/03%20-%20Image_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkHz0FVN6F8u",
        "colab_type": "text"
      },
      "source": [
        "<table width=\"100%\">\n",
        "    <tr>\n",
        "        <td valign=\"top\"><img src=\"https://cdn.ymaws.com/www.abdominalradiology.org/graphics/logo.jpg\"/></td>\n",
        "        <td valign=\"middle\" align=\"right\"><h1>SAR 2020<br/>AI Masters Class</h1></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td align=\"center\" colspan=2><h1>Image Classifier</h1></td>\n",
        "    </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE8-a8NmyNGR",
        "colab_type": "text"
      },
      "source": [
        "**CHEST XRAY CLASSIFIER**\n",
        "\n",
        "Let's build an image classifier from scratch and see if we can use it to differentiate frontal from lateral chest x-rays.\n",
        "\n",
        "Frontal and lateral chest x-rays are so similar within-class and so different between classes that differentiating them is a trivial task for a neural network. But, you can use the *exact same code* to train the classifier to differentiate any other classes of images:\n",
        "\n",
        "*   Pneumothorax vs pneumonia vs normal\n",
        "*   Stroke vs no stroke\n",
        "*   HCC vs adenoma\n",
        "*   Hot dog vs not a hot dog\n",
        "\n",
        "The more subtle the differences between your classes, the more training data (and time) you'll need.\n",
        "\n",
        "---\n",
        "\n",
        "This tutorial is based on Lesson 1 of Practical Deep Learning for Coders v3, a free course offering from [fast.ai](https://course.fast.ai/). I strongly encourage anyone interested to head over to fast.ai to learn more - it's the best resource out there for learning and getting up to speed on image classification as well as more advanced tasks like object detection, image segmentation, and natural language processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCxnQYu5EF7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the required fastai modules\n",
        "!pip3 install fastai | grep -v 'already satisfied'\n",
        "from fastai.vision import *\n",
        "from fastai.metrics import error_rate\n",
        "from fastai.callbacks.hooks import *\n",
        "from fastai.imports import *\n",
        "from fastai import *\n",
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CygcMgenCkXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean out any old data just to be sure, such as if re-running cells\n",
        "!rm -rf images\n",
        "!rm -rf sample_data # Google supplies this but not needed\n",
        "\n",
        "# Download the CXRs for training\n",
        "!wget -q --no-check-certificate 'https://www.dropbox.com/s/p32oela6ac63d7e/cxr.zip' -O ./cxr.zip\n",
        "!mkdir images\n",
        "!cd images; unzip -q \"../cxr.zip\" \n",
        "!rm -rf ./images/__MACOSX\n",
        "!ls images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK8BKS3wtkiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the path to our image directory to a variable named path\n",
        "path = Path('/content/images/cxr/')\n",
        "# get_image_files is a convenience function from fastai.vision that looks in `path` and returns a list of all image files it finds\n",
        "filenames = get_image_files(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbpsGOQZsICK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(filenames[99])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_GwBZsHz_JF",
        "colab_type": "text"
      },
      "source": [
        "As you can tell from the example filename, the *image class* is encoded in the file name itself. This is a common method of labeling images for machine learning - it ensures that the correct label is always associated with each image rather than in a separate file.\n",
        "\n",
        "The images are all named in a consistent way:\n",
        "> `{class}_{serial number}.jpg`\n",
        "\n",
        "> `frontal_0001.jpg`, `lateral_0056.jpg`, etc.\n",
        "\n",
        "---\n",
        "**Protip**\n",
        "\n",
        "When you can identify a *pattern* that isolates the text you want from a longer string, you can use *regular expressions*, or *RegEx*, to extract the text. The RegEx pattern to extract the class (frontal or lateral) from the full file path ('/content/images/cxr/lateral_0062.jpg') is:\n",
        "\n",
        "\n",
        "> **`/([^/]+)_\\d+.jpg$`**\n",
        "\n",
        "\n",
        "We'll save this RegEx pattern as a variable called `pattern`.\n",
        "\n",
        "Learn more about RegEx and pratice at [Pythex.org](https://pythex.org/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui-f4z6-0RRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern = re.compile(r'/([^/]+)_\\d+.jpg$')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_-_FuC20UL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set required arguments for the fastai ImageDataBunch\n",
        "validation_percentage=0.5 #We'll do a 50:50 split: train on 50%, validate on 50%\n",
        "batchsize = 8 # Network weights updated after each batch. Size depends on memory of GPU and image size\n",
        "imagesize=224 # Images will be resized to 224x224 px\n",
        "transforms = get_transforms() # Apply random image transformations: horizontal flip, small rotations, etc. Multiplies the number of different images available\n",
        "np.random.seed(25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERzQpFjvDN9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An ImageDataBunch is a fastai data construct that assembles the images and required settings\n",
        "# and prepares them for loading in to the neural network. \n",
        "data = ImageDataBunch.from_name_re(\n",
        "    path, \n",
        "    filenames, \n",
        "    pattern, \n",
        "    valid_pct=validation_percentage, \n",
        "    ds_tfms = transforms, \n",
        "    size=imagesize, \n",
        "    bs=batchsize).normalize(imagenet_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajs0QFo80h3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can look at the ImageDataBunch and see that it contains separate training and validation datasets\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgyjvAAN0nxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The ImageDataBunch has two classes [frontal, lateral], adn 50 images each in the training and validation datasets\n",
        "data.classes, data.c, len(data.train_ds), len(data.valid_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e10lFyR30rO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can display one batch of 8 images with their associates ground-truth labels\n",
        "# Note that some of the images have been arbitrarily flipped horizontally\n",
        "data.show_batch(rows=3, figsize=(10,8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "459DWI6I1f8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the neural network learner by passing it our ImageDataBunch\n",
        "# Note that we are basing it on an existing network, called \n",
        "# Resnet34 is pretrained on ImageNet - we'll be modifying the pretrained network to learn about CXRs\n",
        "learn = cnn_learner(\n",
        "    data,\n",
        "    models.resnet34,\n",
        "    metrics=(error_rate, accuracy)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4RnVd_T1WwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finally we'll start training the network!\n",
        "# We will have it go through all 50 training images 4 times\n",
        "# Each time through the entire training set is referred to as an epoch\n",
        "# Remember that we defined a batch as 8 images, so after every 8 images the network will adjust its settings\n",
        "# After each epoch it will report back it's current error rate and accuracy\n",
        "learn.fit_one_cycle(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwoDrYxg2oFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can save the trained model and use it later to evaluate new CXRs\n",
        "learn.save('cxr-frontlat-stage1')\n",
        "learn.export()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfgX0dew1xqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The goal of a training a neural network is to \"minimize the loss function\" - the loss function is a\n",
        "# mathematical formula that quantifies how bad the network is at its assigned task.\n",
        "# After each batch, the network measures how bad it is and adjusts its parameters in a direction that leads\n",
        "# to a lower result to the loss function.\n",
        "learn.recorder.plot_losses()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOwYxEs91_ii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can visualize the cases that the network got wrong or was right but less confident in\n",
        "interp = ClassificationInterpretation.from_learner(learn)\n",
        "interp.plot_top_losses(9, figsize=(15,11), heatmap=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQc8zKfF2G2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A confusion matrix plots predicted vs actual\n",
        "# It is most useful when there are several classes and you can see which classes it is confusing for which others\n",
        "interp.plot_confusion_matrix(figsize=(3,3), dpi=200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-nvewd54LIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download a brand-new batch of chest x-rays unrelated to those used for training\n",
        "!wget --no-check-certificate 'https://www.dropbox.com/s/639j1pbq12gs107/palat.zip' -O ./palat.zip\n",
        "\n",
        "!cd images; unzip -q \"../palat.zip\" \n",
        "!rm -rf ./images/__MACOSX\n",
        "!ls images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu5yvrSe2uey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the learner (the fully trained network) and apply it to the new images\n",
        "learn = load_learner('/content/images/cxr/', test=ImageImageList.from_folder('/content/images/palat/test/'))\n",
        "pred,y = learn.get_preds(ds_type=DatasetType.Test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EpDjKnn4gij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at a snapshot of predictions - we can see that it is a list where each entry is two numbers -\n",
        "# the liklihood the network assigns to the given CXR being a PA or lateral.\n",
        "# The higher number is considered to be the class assignment for that CXR\n",
        "pred[200:210]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iXZg-eu4met",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ims = learn.data.test_ds.x\n",
        "classes = ['frontal','lateral']\n",
        "# Argmax simply chooses the index of the highest number of the available options\n",
        "# In this case it choose the higher number for each row in the predictions list\n",
        "lbls = np.argmax(pred, axis=1)\n",
        "rows = 40\n",
        "cols = 10\n",
        "figsize=(20,70)\n",
        "fig,axes = plt.subplots(rows,cols,figsize=figsize)\n",
        "fig.suptitle('predictions', weight='bold',size=14)\n",
        "for idx,im in enumerate(ims):\n",
        "  im.show(ax=axes.flat[idx], title=classes[lbls[idx]])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}